{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:23:24.150056Z",
     "start_time": "2023-07-22T21:23:24.068292Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from geopy import distance\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:23:27.939577Z",
     "start_time": "2023-07-22T21:23:24.088490Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_set_dirty = pd.read_csv('../Data_Cleanup/inCSV/Weather_LosAngeles.csv', sep=\",\")\n",
    "weather_set_dirty['timestamp'] = pd.to_datetime(weather_set_dirty['timestamp'])\n",
    "\n",
    "mean_values_by_day = weather_set_dirty.groupby(weather_set_dirty[\"timestamp\"].dt.date).mean(numeric_only=True).round(1)\n",
    "\n",
    "for col in mean_values_by_day.columns:\n",
    "    weather_set_dirty[col] = weather_set_dirty.apply(\n",
    "        lambda row: mean_values_by_day.loc[row['timestamp'].date(), col]\n",
    "        if pd.isnull(row[col]) else row[col],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "weather_set_dirty['cloud_cover_description'].fillna(method='ffill',inplace=True)\n",
    "\n",
    "weather_set_dirty.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:30:16.576659Z",
     "start_time": "2023-07-22T21:23:27.946575Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bike_set_dirty = pd.read_csv('../Data_Cleanup/inCSV/metro_2017_2022.csv', sep=',', index_col=0)\n",
    "\n",
    "bike_set_dirty.sort_values('start_station_id')\n",
    "bike_set_dirty['start_station_lat'].fillna(method=\"ffill\", inplace=True)\n",
    "bike_set_dirty['start_station_lon'].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "bike_set_dirty.sort_values('end_station_id')\n",
    "bike_set_dirty['end_station_lat'].fillna(method=\"ffill\", inplace=True)\n",
    "bike_set_dirty['end_station_lon'].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "bike_set_dirty_agg_start = bike_set_dirty.groupby('start_station_id')[['start_station_lat', 'start_station_lon']].agg(lambda x: x.mode().iloc[0])\n",
    "bike_set_dirty_agg_end = bike_set_dirty.groupby('end_station_id')[['end_station_lat', 'end_station_lon']].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "bike_set_dirty_merge = bike_set_dirty.merge(bike_set_dirty_agg_start, on='start_station_id', suffixes=('','_mode'))\n",
    "bike_set_dirty_merged = bike_set_dirty_merge.merge(bike_set_dirty_agg_end, on='end_station_id', suffixes=('','_mode'))\n",
    "\n",
    "dateformat0 = \"%H/%M/%S %d-%m-%Y\"\n",
    "dateformat1 = \"%Y-%m-%d %H:%M:%S\"\n",
    "for i, row in bike_set_dirty_merged.iterrows():\n",
    "    bike_set_dirty_merged.at[i, 'start_station_lat'] = bike_set_dirty_merged.loc[i, 'start_station_lat_mode']\n",
    "    bike_set_dirty_merged.at[i, 'start_station_lon'] = bike_set_dirty_merged.loc[i, 'start_station_lon_mode']\n",
    "    bike_set_dirty_merged.at[i, 'end_station_lat'] = bike_set_dirty_merged.loc[i, 'end_station_lat_mode']\n",
    "    bike_set_dirty_merged.at[i, 'end_station_lon'] = bike_set_dirty_merged.loc[i, 'end_station_lon_mode']\n",
    "\n",
    "    try:\n",
    "        bike_set_dirty_merged.at[i, 'start_time'] = datetime.strptime(bike_set_dirty_merged.loc[i, 'start_time'], dateformat0)\n",
    "    except ValueError:\n",
    "        bike_set_dirty_merged.at[i, 'start_time'] = datetime.strptime(bike_set_dirty_merged.loc[i, 'start_time'], dateformat1)\n",
    "    try:\n",
    "        bike_set_dirty_merged.at[i, 'end_time'] = datetime.strptime(bike_set_dirty_merged.loc[i, 'end_time'], dateformat0)\n",
    "    except ValueError:\n",
    "        bike_set_dirty_merged.at[i, 'end_time'] = datetime.strptime(bike_set_dirty_merged.loc[i, 'end_time'], dateformat1)\n",
    "\n",
    "bike_set_almost_clean = bike_set_dirty_merged\n",
    "bike_set_clean = bike_set_almost_clean.drop(['start_station_lat_mode', 'start_station_lon_mode', 'end_station_lat_mode', 'end_station_lon_mode'], axis=1)\n",
    "bike_set_clean['start_time'] = pd.to_datetime(bike_set_almost_clean['start_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "bike_set_clean['end_time'] = pd.to_datetime(bike_set_almost_clean['end_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "bike_set_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:32:12.215533Z",
     "start_time": "2023-07-22T21:30:16.577217Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metro = bike_set_clean\n",
    "metro['idle_time'] = None\n",
    "metro = metro.sort_values(by='end_time')\n",
    "\n",
    "BigDict = {\n",
    "    # station_id: {\n",
    "    #     transaction_id: end_time\n",
    "    # }\n",
    "}\n",
    "\n",
    "for i, row in metro.iterrows():\n",
    "    current_start_time = row[\"start_time\"]\n",
    "    current_end_time = row[\"end_time\"]\n",
    "    current_end_station_id = row[\"end_station_id\"]\n",
    "    current_start_station_id = row[\"start_station_id\"]\n",
    "    current_bike_id = row[\"bike_id\"]\n",
    "    current_transaction_id = i\n",
    "\n",
    "    if current_start_station_id in BigDict:\n",
    "        for transaction in list(BigDict[current_start_station_id]):  # Create a copy of the keys\n",
    "            if current_start_time < BigDict[current_start_station_id][transaction]:\n",
    "                continue\n",
    "            if metro.at[transaction, \"idle_time\"] is not None:\n",
    "                continue\n",
    "            idle_time = current_start_time - BigDict[current_start_station_id][transaction]\n",
    "            metro.at[transaction, \"idle_time\"] = idle_time\n",
    "            #if idle_time.total_seconds() >= 0:\n",
    "            del BigDict[current_start_station_id][transaction]\n",
    "\n",
    "    if current_end_station_id not in BigDict:\n",
    "        BigDict[current_end_station_id] = {current_transaction_id: current_end_time}\n",
    "\n",
    "    BigDict[current_end_station_id][current_transaction_id] = current_end_time\n",
    "\n",
    "metro.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:34.353329Z",
     "start_time": "2023-07-22T21:32:12.223047Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metro = metro.sort_values(by='start_time')\n",
    "#Create the corresponding columns\n",
    "metro[\"morgens\"] = 0\n",
    "metro[\"vormittags\"] = 0\n",
    "metro[\"mittags\"] = 0\n",
    "metro[\"nachmittags\"]=0\n",
    "metro[\"abends\"]=0\n",
    "metro[\"nachts\"]=0\n",
    "\n",
    "#set the start and end of each day time\n",
    "morgens = range(5,9)\n",
    "vormittags = range(9,12)\n",
    "mittags = range(12,15)\n",
    "nachmittags = range(15,19)\n",
    "abends = range(19, 23)\n",
    "nachts = [23,0,1,2,3,4]\n",
    "\n",
    "#This list will serve as string provider that we can insert later, to access the proper column\n",
    "TagesZeiten = [\"morgens\", \"vormittags\", \"mittags\", \"nachmittags\", \"abends\", \"nachts\"]\n",
    "#This list will serve to provide the start time of each day time, so we know when to change columns\n",
    "Intervall_starts = [5,9,12,15,19,23]\n",
    "\n",
    "metro[\"monday\"] = 0\n",
    "metro[\"tuesday\"] = 0\n",
    "metro[\"wednesday\"] = 0\n",
    "metro[\"thursday\"] = 0\n",
    "metro[\"friday\"] = 0\n",
    "metro[\"saturday\"] = 0\n",
    "metro[\"sunday\"] = 0\n",
    "\n",
    "Days = [\"monday\",\"tuesday\",\"wednesday\", \"thursday\",\"friday\",\"saturday\", \"sunday\"]\n",
    "\n",
    "#Make sure to change the \"idle_time\" to a timedelta object, so we can use .totalseconds() later\n",
    "metro[\"idle_time\"] = pd.to_timedelta(metro[\"idle_time\"])\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for i, row in metro.iterrows():\n",
    "    end_date = row[\"end_time\"] + row[\"idle_time\"]\n",
    "    #we only need the starting time hour\n",
    "    end_hour = end_date.hour\n",
    "    # and we need to know the duration (idle time) we need to look back\n",
    "    idle_time = row[\"idle_time\"]\n",
    "    # make sure to skip any rows with no idle times, since they have 0s in the day times columns\n",
    "    try:\n",
    "        NumHours = float(idle_time.total_seconds() / 3600)\n",
    "        if end_hour in morgens:\n",
    "            index = 0\n",
    "        elif end_hour in vormittags:\n",
    "            index = 1\n",
    "        elif end_hour in mittags:\n",
    "            index = 2\n",
    "        elif end_hour in nachmittags:\n",
    "            index = 3\n",
    "        elif end_hour in abends:\n",
    "            index = 4\n",
    "        elif end_hour in nachts:\n",
    "            index = 5\n",
    "\n",
    "        while NumHours >0 and index > -6:\n",
    "            metro.at[i, TagesZeiten[index]] = 1\n",
    "            DistanceToIntervallStart = abs(Intervall_starts[index]- end_hour)\n",
    "\n",
    "            NumHours -= DistanceToIntervallStart\n",
    "            end_hour -= DistanceToIntervallStart\n",
    "            index -= 1\n",
    "\n",
    "        NumDays = float(row[\"idle_time\"].total_seconds() / (24 * 60 * 60))\n",
    "\n",
    "        day = end_date.weekday()\n",
    "\n",
    "        while NumDays > 0 and day > -7:\n",
    "            metro.at[i, Days[day]] = 1\n",
    "\n",
    "            NumDays -= 1\n",
    "            day -= 1\n",
    "\n",
    "    except ValueError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:34.387950Z",
     "start_time": "2023-07-22T21:34:34.355326Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_set = weather_set_dirty\n",
    "metro_set = metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:34.603573Z",
     "start_time": "2023-07-22T21:34:34.385944Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_set.info(), metro_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:36.227242Z",
     "start_time": "2023-07-22T21:34:34.605574Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metro_set.sort_values('start_time', inplace=True)\n",
    "weather_set.sort_values('timestamp', inplace=True)\n",
    "\n",
    "Weather_Set1 = weather_set.set_index('timestamp').reindex(metro_set.set_index('start_time').index, method='nearest')\n",
    "Weather_Set1 = Weather_Set1.loc[~Weather_Set1.index.duplicated(keep='first')].reset_index()\n",
    "\n",
    "merged_set = pd.merge(metro_set, Weather_Set1, on='start_time', how='left')\n",
    "merged_set = merged_set.drop(['city'], axis=1)\n",
    "merged_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:36.739309Z",
     "start_time": "2023-07-22T21:34:36.255246Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_set_descriptive = merged_set\n",
    "mean_idle_time = merged_set_descriptive.groupby(\"end_station_id\")[\"idle_time\"].mean()\n",
    "\n",
    "count_morning = merged_set_descriptive.groupby(\"end_station_id\")[\"morgens\"].sum()\n",
    "count_vormittag = merged_set_descriptive.groupby(\"end_station_id\")[\"vormittags\"].sum()\n",
    "count_midday = merged_set_descriptive.groupby(\"end_station_id\")[\"mittags\"].sum()\n",
    "count_afternoon = merged_set_descriptive.groupby(\"end_station_id\")[\"nachmittags\"].sum()\n",
    "count_evening = merged_set_descriptive.groupby(\"end_station_id\")[\"abends\"].sum()\n",
    "count_night = merged_set_descriptive.groupby(\"end_station_id\")[\"nachts\"].sum()\n",
    "count_mondays = merged_set_descriptive.groupby(\"end_station_id\")[\"monday\"].sum()\n",
    "count_tuesdays = merged_set_descriptive.groupby(\"end_station_id\")[\"tuesday\"].sum()\n",
    "count_wednesdays = merged_set_descriptive.groupby(\"end_station_id\")[\"wednesday\"].sum()\n",
    "count_thursdays = merged_set_descriptive.groupby(\"end_station_id\")[\"thursday\"].sum()\n",
    "count_fridays = merged_set_descriptive.groupby(\"end_station_id\")[\"friday\"].sum()\n",
    "count_saturdays = merged_set_descriptive.groupby(\"end_station_id\")[\"saturday\"].sum()\n",
    "count_sundays = merged_set_descriptive.groupby(\"end_station_id\")[\"sunday\"].sum()\n",
    "station_lat = merged_set_descriptive.groupby(\"end_station_id\")[\"end_station_lat\"].unique()\n",
    "station_lon = merged_set_descriptive.groupby(\"end_station_id\")[\"end_station_lon\"].unique()\n",
    "\n",
    "result = pd.concat([mean_idle_time, count_morning, count_vormittag, count_midday, count_afternoon, count_evening, count_night, count_mondays, count_tuesdays, count_wednesdays, count_thursdays, count_fridays, count_saturdays, count_sundays, station_lat, station_lon], axis=1)\n",
    "station_ID_with_AvgIdleDaytimesDays_and_lonlat = result.dropna(subset=[\"idle_time\"])\n",
    "station_ID_with_AvgIdleDaytimesDays_and_lonlat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:40.772024Z",
     "start_time": "2023-07-22T21:34:36.753491Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = merged_set_descriptive.copy()\n",
    "\n",
    "data['start_time'] = pd.to_datetime(data['start_time'])\n",
    "data['end_time'] = pd.to_datetime(data['end_time'])\n",
    "data['trip_duration'] = (data['end_time'] - data['start_time']).dt.total_seconds() / 60\n",
    "\n",
    "data['hour'] = data['start_time'].dt.hour\n",
    "data['weekday'] = data['start_time'].dt.day_name()\n",
    "data['day'] = data['start_time'].dt.day\n",
    "data['month'] = data['start_time'].dt.month_name()\n",
    "data['year'] = data['start_time'].dt.year\n",
    "\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "data['weekday'] = pd.Categorical(data['weekday'], categories=weekday_order, ordered=True)\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "data['month'] = pd.Categorical(data['month'], categories=month_order, ordered=True)\n",
    "\n",
    "usage_by_hour = data['hour'].value_counts().sort_index()\n",
    "usage_by_weekday = data['weekday'].value_counts().sort_index()\n",
    "usage_by_day = data['day'].value_counts().sort_index()\n",
    "usage_by_month = data['month'].value_counts().sort_index()\n",
    "usage_by_year = data['year'].value_counts().sort_index()\n",
    "\n",
    "usage_by_start_station = data['start_station_id'].value_counts().head(10)\n",
    "usage_by_end_station = data['end_station_id'].value_counts().head(10)\n",
    "\n",
    "bins = [0, 10, 20, 30, 40, 50]\n",
    "labels = ['0-10', '10-20', '20-30', '30-40', '40-50']\n",
    "data['temperature_bin'] = pd.cut(data['temperature'], bins=bins, labels=labels)\n",
    "usage_by_temperature = data['temperature_bin'].value_counts()\n",
    "\n",
    "\n",
    "bins = [-np.inf, 0, 0.5, 2.5, 7.6, np.inf]\n",
    "labels = ['No', 'Very Light', 'Light', 'Moderate', 'Heavy']\n",
    "data['precipitation_category'] = pd.cut(data['precipitation'], bins=bins, labels=labels)\n",
    "usage_by_precipitation = data['precipitation_category'].value_counts()\n",
    "\n",
    "data_with_precipitation = data[data['precipitation'] > 0].copy()\n",
    "bins = [0, 0.5, 2.5, 7.6, np.inf]\n",
    "labels = ['Very Light', 'Light', 'Moderate', 'Heavy']\n",
    "data_with_precipitation['precipitation_category'] = pd.cut(data_with_precipitation['precipitation'], bins=bins, labels=labels)\n",
    "usage_by_precipitation2 = data_with_precipitation['precipitation_category'].value_counts()\n",
    "\n",
    "cloud_cover_categories = ['Light Rain', 'Rain', 'Mostly Cloudy', 'Partly Cloudy', 'Fair', 'Cloudy', 'Heavy Rain', 'Fog', 'Haze', 'Heavy Rain / Windy']\n",
    "data['cloud_cover_description'] = pd.Categorical(data['cloud_cover_description'], categories=cloud_cover_categories, ordered=True)\n",
    "usage_by_cloud_cover = data['cloud_cover_description'].value_counts().sort_index()\n",
    "\n",
    "data_without_fair = data[data['cloud_cover_description'] != 'Fair'].copy()\n",
    "cloud_cover_categories2 = ['Light Rain', 'Rain', 'Mostly Cloudy', 'Partly Cloudy', 'Cloudy', 'Heavy Rain', 'Fog', 'Haze', 'Heavy Rain / Windy']\n",
    "data_without_fair['cloud_cover_description'] = pd.Categorical(data_without_fair['cloud_cover_description'], categories=cloud_cover_categories2, ordered=True)\n",
    "usage_by_cloud_cover2 = data_without_fair['cloud_cover_description'].value_counts().sort_index()\n",
    "\n",
    "def bar_plot(usage, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(15,6))\n",
    "    sns.barplot(x=usage.index, y=usage.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "bar_plot(usage_by_hour, 'Usage by Hour of the Day', 'Hour of the Day', 'Number of Trips')\n",
    "bar_plot(usage_by_weekday, 'Usage by Day of the Week', 'Day of the Week', 'Number of Trips')\n",
    "bar_plot(usage_by_day, 'Usage by Day', 'Day', 'Number of Trips')\n",
    "bar_plot(usage_by_month, 'Usage by Month', 'Month', 'Number of Trips')\n",
    "bar_plot(usage_by_year, 'Usage by Year', 'Year', 'Number of Trips')\n",
    "bar_plot(usage_by_temperature, 'Usage by Temperature', 'Temperature', 'Number of Trips')\n",
    "bar_plot(usage_by_precipitation, 'Usage by Precipitation', 'Precipitation', 'Number of Trips')\n",
    "bar_plot(usage_by_precipitation2, 'Usage by Precipitation greater than 0', 'Precipitation', 'Number of Trips')\n",
    "bar_plot(usage_by_cloud_cover, 'Usage by Cloud Cover Description', 'Cloud Cover Description', 'Number of Trips')\n",
    "bar_plot(usage_by_cloud_cover2, 'Usage by Cloud Cover Description Without Fair', 'Cloud Cover Description', 'Number of Trips')\n",
    "bar_plot(usage_by_start_station.astype(str), 'Top 10 Start Stations', 'Start Station', 'Number of Trips')\n",
    "bar_plot(usage_by_end_station.astype(str), 'Top 10 End Stations', 'End Station', 'Number of Trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:40.871164Z",
     "start_time": "2023-07-22T21:34:40.774000Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.info, result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_df = result.sort_values(by='idle_time', ascending=False)\n",
    "\n",
    "print(sorted_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the station 4403 has an unnormal idle_time, my first idea would be to filter the original data by the station id and plot the idle_time of each trip against the start time of the trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_data = merged_set_descriptive.copy()\n",
    "station_data = full_data[(full_data['start_station_id'] == 4403) | (full_data['end_station_id'] == 4403)]\n",
    "station_data['idle_time'] = pd.to_timedelta(station_data['idle_time']).dt.total_seconds() / (24 * 60 * 60)\n",
    "\n",
    "plt.hist(station_data['idle_time'], bins=20)\n",
    "plt.xlabel('Idle Time (days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Idle Time for Station 4403')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the Histogramm we can see that we have around 40 rows that has more than 1000 days in idle time.\n",
    "\n",
    "Lets take a better look at those 40 trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "station_data = full_data[(full_data['start_station_id'] == 4403) | (full_data['end_station_id'] == 4403)]\n",
    "station_data['start_time'] = pd.to_datetime(station_data[\"start_time\"])\n",
    "station_data['end_time'] = pd.to_datetime(station_data[\"end_time\"])\n",
    "station_data['idle_time'] = pd.to_timedelta(station_data['idle_time'])\n",
    "long_idle_trips = station_data[station_data[\"idle_time\"] > pd.Timedelta(days=1000)]\n",
    "long_idle_trips = long_idle_trips.sort_values('end_time')\n",
    "comparison_results = pd.DataFrame(columns=['Trip', 'End Time', 'Next Start Time'])\n",
    "for index, row in long_idle_trips.iterrows():\n",
    "    end_time = row['end_time']\n",
    "    for index, row in station_data.iterrows():\n",
    "        if row[\"start_time\"] > end_time:\n",
    "            next_start_time = row[\"start_time\"]\n",
    "    comparison_results = pd.concat([comparison_results, pd.DataFrame({'Trip': [index], 'End Time': [end_time], 'Next Start Time': [next_start_time]})], ignore_index=True)\n",
    "\n",
    "print(comparison_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems tht there has been a huge break in the usage in this station, let try to find it.\n",
    "\n",
    "Looking at the data we can see that no one used the station between 2020-02-23 15:09:00 and 2022-12-04 14:54:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = sorted_df[sorted_df['idle_time'] <= pd.Timedelta(days=30)]\n",
    "top_20_stations = filtered_df.head(20)\n",
    "top_20_stations = top_20_stations.reset_index()\n",
    "top_20_stations[\"end_station_id\"] = top_20_stations[\"end_station_id\"].astype(str)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_20_stations[\"end_station_id\"], top_20_stations[\"idle_time\"].dt.days)\n",
    "plt.title(\"Top 20 Stations with Highest Idle Time\")\n",
    "plt.xlabel(\"Station ID\")\n",
    "plt.ylabel(\"Idle Time (Days)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bottom_20_stations = filtered_df.tail(20)\n",
    "bottom_20_stations = bottom_20_stations.reset_index()\n",
    "bottom_20_stations[\"end_station_id\"] = bottom_20_stations[\"end_station_id\"].astype(str)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bottom_20_stations[\"end_station_id\"], bottom_20_stations[\"idle_time\"].dt.total_seconds()/60)\n",
    "plt.title(\"Bottom 20 Stations with Lowest Idle Time\")\n",
    "plt.xlabel(\"Station ID\")\n",
    "plt.ylabel(\"Idle Time (Minutes)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(filtered_df.count())\n",
    "filtered_df = filtered_df.reset_index()\n",
    "top_50_percent = filtered_df.head(197)\n",
    "bottom_50_percent = filtered_df.tail(196)\n",
    "\n",
    "\n",
    "\n",
    "avg_idle_time_top_50 = top_50_percent[\"idle_time\"].mean()\n",
    "avg_idle_time_bottom_50 = bottom_50_percent[\"idle_time\"].mean()\n",
    "over_top_avg_df = top_50_percent[top_50_percent[\"idle_time\"] >= avg_idle_time_top_50]\n",
    "under_bottom_avg_df = bottom_50_percent[bottom_50_percent[\"idle_time\"] <= avg_idle_time_bottom_50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "over_top_avg_df[\"neighboring_stations\"] = None\n",
    "over_top_avg_df[\"avg_neighbor_idle_time\"] = None\n",
    "\n",
    "for i, row in over_top_avg_df.iterrows():\n",
    "    station_id = row['end_station_id']\n",
    "    lat = row['end_station_lat']\n",
    "    lon = row['end_station_lon']\n",
    "    center_coords = (lat, lon)\n",
    "    neighbor_idle_time_sum = pd.Timedelta(0)\n",
    "    neighboring_stations = []\n",
    "    for index, sorted_row in filtered_df.iterrows():\n",
    "        station_coords = (sorted_row['end_station_lat'], sorted_row['end_station_lon'])\n",
    "        dist = distance.distance(center_coords, station_coords).meters\n",
    "        if dist <= 500:\n",
    "            neighboring_stations.append(sorted_row[\"end_station_id\"])\n",
    "            neighbor_idle_time_sum += pd.Timedelta(sorted_row[\"idle_time\"])\n",
    "    over_top_avg_df.at[i, \"neighboring_stations\"] = neighboring_stations\n",
    "    over_top_avg_df.at[i, \"avg_neighbor_idle_time\"] = neighbor_idle_time_sum / len(neighboring_stations)\n",
    "\n",
    "\n",
    "under_bottom_avg_df[\"neighboring_stations\"] = None\n",
    "under_bottom_avg_df[\"avg_neighbor_idle_time\"] = None\n",
    "for i, row in under_bottom_avg_df.iterrows():\n",
    "    station_id = row['end_station_id']\n",
    "    lat = row['end_station_lat']\n",
    "    lon = row['end_station_lon']\n",
    "    center_coords = (lat, lon)\n",
    "    neighbor_idle_time_sum = pd.Timedelta(0)\n",
    "    neighboring_stations = []\n",
    "    for index, sorted_row in filtered_df.iterrows():\n",
    "        station_coords = (sorted_row['end_station_lat'], sorted_row['end_station_lon'])\n",
    "        dist = distance.distance(center_coords, station_coords).meters\n",
    "        if dist <= 500:\n",
    "            neighboring_stations.append(sorted_row[\"end_station_id\"])\n",
    "            neighbor_idle_time_sum += pd.Timedelta(sorted_row[\"idle_time\"])\n",
    "\n",
    "    under_bottom_avg_df.at[i, \"neighboring_stations\"] = neighboring_stations\n",
    "    under_bottom_avg_df.at[i, \"avg_neighbor_idle_time\"] = neighbor_idle_time_sum / len(neighboring_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_be_removed = []\n",
    "for i, zeile in over_top_avg_df.iterrows():\n",
    "    if zeile[\"idle_time\"] > zeile[\"avg_neighbor_idle_time\"] and len(zeile[\"neighboring_stations\"]) > 2:\n",
    "        to_be_removed.append(zeile[\"end_station_id\"])\n",
    "\n",
    "to_be_expanded = []\n",
    "for i, zeile in under_bottom_avg_df.iterrows():\n",
    "    if zeile[\"idle_time\"] < zeile[\"avg_neighbor_idle_time\"] and len(zeile[\"neighboring_stations\"]) < 3:\n",
    "        to_be_expanded.append(zeile[\"end_station_id\"])\n",
    "\n",
    "print(to_be_removed)\n",
    "print(len(to_be_removed))\n",
    "print(to_be_expanded)\n",
    "print(len(to_be_expanded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "la_coordinates = (34.0522, -118.2437)\n",
    "map_la = folium.Map(location=la_coordinates, zoom_start=11)\n",
    "for index, row in filtered_df.iterrows():\n",
    "    station_id = row['end_station_id']\n",
    "    lat = row['end_station_lat']\n",
    "    lon = row['end_station_lon']\n",
    "    if station_id in to_be_expanded:\n",
    "        folium.Marker(location=[lat, lon], tooltip=[str(station_id)], icon = folium.Icon(color=\"green\")).add_to(map_la)\n",
    "    elif station_id in to_be_removed:\n",
    "        folium.Marker(location=[lat, lon], tooltip=[str(station_id)], icon = folium.Icon(color=\"red\")).add_to(map_la)\n",
    "    else:\n",
    "        folium.Marker(location=[lat, lon], tooltip=[str(station_id)]).add_to(map_la)\n",
    "map_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:41.535338Z",
     "start_time": "2023-07-22T21:34:40.790569Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_set_pred = merged_set.dropna(subset=[\"idle_time\"])\n",
    "merged_set_pred['idle_time'] = merged_set_pred['idle_time'].dt.total_seconds()/3600\n",
    "merged_set_pred['end_hour'] = merged_set_pred['end_time'].dt.hour\n",
    "\n",
    "le = LabelEncoder()\n",
    "merged_set_pred['cloud_cover_description'] = le.fit_transform(merged_set_pred['cloud_cover_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:34:43.487337Z",
     "start_time": "2023-07-22T21:34:41.509984Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "y = merged_set_pred['idle_time'].values\n",
    "\n",
    "# Define features\n",
    "X = merged_set_pred.drop(['start_time', 'end_time', 'bike_id', 'idle_time', 'start_station_id','start_station_lat','start_station_lon'], axis=1)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Perform train-validation-test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X[feature_names], y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.285, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_train_val = scaler.transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:26:33.974841Z",
     "start_time": "2023-07-22T20:13:11.118828Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the parameter lgbm_grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'num_leaves': [31, 62, 93],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Create a LightGBM model\n",
    "lgbm_model = lgb.LGBMRegressor()\n",
    "\n",
    "# Create the lgbm_grid search object\n",
    "lgbm_grid = GridSearchCV(lgbm_model, param_grid, cv=5, scoring='r2')\n",
    "\n",
    "# Fit the lgbm_grid search object to the data\n",
    "lgbm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(lgbm_grid.best_params_)\n",
    "print(lgbm_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:02:04.308690Z",
     "start_time": "2023-07-22T20:26:33.974841Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the parameter tree_grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create a DecisionTreeRegressor model\n",
    "decTree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Create the tree_grid search object\n",
    "tree_grid = GridSearchCV(decTree_model, param_grid, cv=5, scoring='r2')\n",
    "\n",
    "# Fit the tree_grid search object to the data\n",
    "tree_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(tree_grid.best_params_)\n",
    "print(tree_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred_tree = tree_grid.best_estimator_.predict(X_val)\n",
    "y_pred_lgbm = lgbm_grid.best_estimator_.predict(X_val)\n",
    "\n",
    "mse_tree = mean_squared_error(y_val, y_pred_tree)\n",
    "mae_tree = mean_absolute_error(y_val, y_pred_tree)\n",
    "r2_tree = r2_score(y_val, y_pred_tree)\n",
    "mse_lgbm = mean_squared_error(y_val, y_pred_lgbm)\n",
    "mae_lgbm = mean_absolute_error(y_val, y_pred_lgbm)\n",
    "r2_lgbm = r2_score(y_val, y_pred_lgbm)\n",
    "\n",
    "models = ['Decision Tree', 'LightGBM']\n",
    "mse_values = [mse_tree, mse_lgbm]\n",
    "mae_values = [mae_tree, mae_lgbm]\n",
    "r2_values = [r2_tree, r2_lgbm]\n",
    "\n",
    "def comparison_plot(values, title, ylabel):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=values, palette='viridis')\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "comparison_plot(mse_values, 'Comparison of Mean Squared Error', 'MSE')\n",
    "comparison_plot(mae_values, 'Comparison of Mean Absolute Error', 'MAE')\n",
    "comparison_plot(r2_values, 'Comparison of R2 Score', 'R2 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:02:26.828643Z",
     "start_time": "2023-07-22T21:02:04.311199Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_params_tree = tree_grid.best_params_\n",
    "best_params_lgbm = lgbm_grid.best_params_\n",
    "base_models = [\n",
    "    ('decision_tree', DecisionTreeRegressor(**best_params_tree)),\n",
    "    ('lgbm', LGBMRegressor(**best_params_lgbm))\n",
    "]\n",
    "\n",
    "# Define the meta model\n",
    "meta_model = Lasso()\n",
    "\n",
    "final_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred_final = final_model.predict(X_val)\n",
    "mse_final = mean_squared_error(y_val, y_pred_final)\n",
    "mae_final = mean_absolute_error(y_val, y_pred_final)\n",
    "r2_final = r2_score(y_val, y_pred_final)\n",
    "\n",
    "models.append('StackingRegressor')\n",
    "mse_values.append(mse_final)\n",
    "mae_values.append(mae_final)\n",
    "r2_values.append(r2_final)\n",
    "\n",
    "comparison_plot(mse_values, 'Comparison of Mean Squared Error', 'MSE')\n",
    "comparison_plot(mae_values, 'Comparison of Mean Absolute Error', 'MAE')\n",
    "comparison_plot(r2_values, 'Comparison of R2 Score', 'R2 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T21:02:28.379913Z",
     "start_time": "2023-07-22T21:02:26.833683Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "val_preds = final_model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, val_preds)\n",
    "val_mae = mean_absolute_error(y_val, val_preds)\n",
    "val_r2 = r2_score(y_val, val_preds)\n",
    "\n",
    "test_preds = final_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds)\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "print('Validation MSE:', val_mse)\n",
    "print('Validation MAE:', val_mae)\n",
    "print('Validation R2:', val_r2)\n",
    "\n",
    "print('Test MSE:', test_mse)\n",
    "print('Test MAE:', test_mae)\n",
    "print('Test R2:', test_r2)\n",
    "\n",
    "labels = ['MSE', 'MAE', 'R2']\n",
    "val_values = [val_mse, val_mae, val_r2]\n",
    "test_values = [test_mse, test_mae, test_r2]\n",
    "\n",
    "#Create a figure with three subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# For each metric, create a bar plot comparing the validation and test values\n",
    "for i in range(3):\n",
    "    ax[i].bar(['Validation', 'Test'], [val_values[i], test_values[i]], color=['blue', 'green'])\n",
    "    ax[i].set_title(labels[i])\n",
    "    ax[i].set_ylabel(labels[i])\n",
    "\n",
    "# Adjust the layout and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
